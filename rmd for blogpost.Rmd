---
title: "Blog Post"
output: md
preserve_yaml: False
---
# Intruduction
I have always had a fascination with the two World Wars. They were springboards for invention and innovation. They inspired millions of men into great acts of courage ans sacrifice. Unfortunately, they were also causes of much devastation and pain. There are few places in world that did not feel their effects. 

One of the best ways to investigate this effect is to examine the loss of life for each nation involved. Luckily, Wikipedia has compiled data sets for the casualty count for both [World War One]("https://en.wikipedia.org/wiki/World_War_II_casualties") and [World War Two]("https://en.wikipedia.org/wiki/World_War_I_casualties"). Let

# Method
The simplest way to gather data from the internet is to use an API. While Wikipedia does have an API, I could not find an easy way to gather information from tables using it. Fortunately, gathering data from Wikipedia tables is a breeze with Python's [Pandas](https://pypi.org/project/pandas/) and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) modules. 

Beautifulsoup is the quintessential tool in python used to scrape web data. However, it can be difficult and confusing to use. Luckily, pandas has a function that is a wrapper for using beautifulsoup to scrape tables. 

## How to
First, install the necessary modules using the following command line arguments.
```
pip install pandas
pip intall beautifulsoup4
```
Once installed, the pandas.read_html() function retrieves all of the table data from a specified url and stores the data in a list of tables.  
```
# Get links
ww2_url= "https://en.wikipedia.org/wiki/World_War_II_casualties"
ww1_url= "https://en.wikipedia.org/wiki/World_War_I_casualties"

#Read tables and select specified one
ww2_df = pd.read_html(ww2_url)[0]
ww1_df = pd.read_html(ww1_url)[0]
ww1_df.columns = ww1_df.columns.droplevel(1)  #remove multi-index layer
```
After a little cleaning after merging the tables, we are left with a data set similar to the following:
```{r, echo=F}
library(tidyverse, warn.conflicts = F, quietly = T, verbose = F)
library(kableExtra, warn.conflicts = F, quietly = T, verbose = F)

data <- read.csv("Uncleaned_data.csv", na.strings = c("", "NA"))
kable(data %>% drop_na() %>% .[-1])
```
You can find the full table at this (link)[FIXME]. Obviously, this data requires a bit more cleaning in order to be usable. However, that is a topic for a different, much more intricate, blog post. 

# Conclusion
I hope that this helps you in your web scraping adventures, and I encourage you to explore more with scraping Wikipedia. Who knows, maybe one of you will figure out how to use the Wikipedia API to get table data! If you do, please reach out and let me know how. ;)




